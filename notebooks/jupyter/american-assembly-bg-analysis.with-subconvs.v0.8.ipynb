{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-ioVEx8CdWz"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pol-is/notebooks/blob/master/020-PCA.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rHx9LnL3CdW1",
    "outputId": "5144fc28-0587-4ac0-ec83-ffd3e2857d65"
   },
   "outputs": [],
   "source": [
    "### Initialization ###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "from textwrap import wrap\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import numba\n",
    "\n",
    "import umap\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dorTrnkCdW7",
    "outputId": "8c20c007-2c03-4da4-9832-e30c5a0ede35"
   },
   "outputs": [],
   "source": [
    "## Set up plots\n",
    "plt.figure(figsize=(500, 500))\n",
    "sns.set_context('poster')\n",
    "sns.set_style('white')\n",
    "sns.set(font_scale=.7)\n",
    "sns.set_color_codes()\n",
    "\n",
    "%matplotlib inline\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDaayAAWCdXA"
   },
   "source": [
    "### Import raw data && clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQUWrHA4CdXA",
    "outputId": "80e6c24a-e9e5-4c6c-a47f-f87b5b4ccc75"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/bowling-green.american-assembly/participants-votes.csv',index_col='participant')\n",
    "df_comments = pd.read_csv('../../data/bowling-green.american-assembly/comments.csv',index_col='comment-id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqfY8aBBCdXD"
   },
   "outputs": [],
   "source": [
    "df_comments.index = df_comments.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwzUxgZ4CdXF"
   },
   "outputs": [],
   "source": [
    "metadata_fields = ['group-id', 'n-comments', 'n-votes', \n",
    "                   'n-agree', 'n-disagree']\n",
    "val_fields = [c for c in df.columns.values if c not in metadata_fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDbIQ2O9CdXI"
   },
   "outputs": [],
   "source": [
    "# remove statements (columns) which were moderated out\n",
    "statements_all_in = sorted(list(df_comments.loc[df_comments[\"moderated\"] > 0].index.array), key = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdLanWjYCdXK"
   },
   "outputs": [],
   "source": [
    "## for a row, count the number of finite values\n",
    "def count_finite(row):\n",
    "    finite = np.isfinite(row[val_fields]) # boolean array of whether each entry is finite\n",
    "    return sum(finite) # count number of True values in `finite`\n",
    "\n",
    "## REMOVE PARTICIPANTS WITH LESS THAN N VOTES check for each row if the number of finite values >= cutoff\n",
    "def select_rows(df, threshold=60):\n",
    "    \n",
    "    number_of_votes = df.apply(count_finite, axis=1)\n",
    "    valid = number_of_votes >= threshold\n",
    "    \n",
    "    return df[valid]\n",
    "    \n",
    "df = select_rows(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azGCtyVqCdXP"
   },
   "outputs": [],
   "source": [
    "metadata = df[metadata_fields]\n",
    "vals = df[val_fields]\n",
    "# If the participant didn't see the statement, it's a null value, here we fill in the nulls with zeros\n",
    "vals = vals.fillna(0)\n",
    "vals = vals.sort_values(\"participant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BAi9JOLCdXU"
   },
   "outputs": [],
   "source": [
    "high_variance = [\n",
    "    \"20\",\"26\",\"90\"\n",
    "]\n",
    "\n",
    "vals_high_variance = vals[high_variance]\n",
    "\n",
    "statements_consensus = [\n",
    "    \"39\",\"200\",\"83\",\"82\",\"737\",\"64\",\"127\",\"126\",\"66\",\n",
    "]\n",
    "\n",
    "vals_consensus = vals[statements_consensus]\n",
    "\n",
    "consensus_disagree = [\n",
    "    \"353\",\"779\",\"720\",\"354\"\n",
    "]\n",
    "\n",
    "vals_consensus_disagree = vals[consensus_disagree]\n",
    "\n",
    "statements_opiods = [\n",
    "    \"9\",\n",
    "    \"19\",\n",
    "    \"11\",\n",
    "    \"10\",\n",
    "    \"675\",\n",
    "    \"753\",\n",
    "    \"742\",\n",
    "    \"336\",\n",
    "    \"329\",\n",
    "    \"683\",\n",
    "    \"686\",\n",
    "    \"894\",\n",
    "    \"885\",\n",
    "    \"846\",\n",
    "    \"688\",\n",
    "    \"679\",\n",
    "    \"678\",\n",
    "    \"676\",\n",
    "]\n",
    "vals_opiods = vals[statements_opiods]\n",
    "\n",
    "\n",
    "statements_homelessness = [\n",
    "    \"24\",\"25\",\"34\",\n",
    "#     \"45\",\n",
    "    \"55\",\n",
    "    \"103\",\"105\",\"106\",\"147\",\n",
    "    \"160\",\"232\",\"233\",\n",
    "    \"404\",\n",
    "#     \"535\", \n",
    "#     \"586\",\n",
    "#     \"782\",\n",
    "#     \"828\",\n",
    "]\n",
    "vals_homelessness = vals[statements_homelessness]\n",
    "\n",
    "                              \n",
    "\n",
    "vals_all_in = vals[statements_all_in]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kK0L_w4cCdXX"
   },
   "source": [
    "# Overall stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKkG2Nk9CdXX"
   },
   "source": [
    "How sparse is the dataset? How much agree, how much disagree, how much pass? Zero is 'passed' or 'did not see the comment to vote on it'. 1 is agree, -1 is disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLzzsooNCdXY",
    "outputId": "3c46a87c-9777-4adf-f5b4-5bdce22d61dc"
   },
   "outputs": [],
   "source": [
    "melted = vals.melt();\n",
    "all_votes = melted.count();\n",
    "by_type = melted[\"value\"].value_counts();\n",
    "total_possible_votes = all_votes[\"value\"];\n",
    "total_agrees = by_type[1.0];\n",
    "total_disagrees = by_type[-1.0];\n",
    "total_without_vote = by_type[0.0];\n",
    "\n",
    "print(\"Dimensions of matrix:\", df.shape)\n",
    "print(\"Dimensions of matrix:\", vals.shape)\n",
    "print(\"Total number of possible votes:\", total_possible_votes)\n",
    "print(\"Total number of agrees:\", total_agrees)\n",
    "print(\"Total number of disagrees:\", total_disagrees)\n",
    "print(\"Total without vote:\", total_without_vote)\n",
    "print(\"Percent sparse: \", total_without_vote / total_possible_votes,\"%\")\n",
    "\n",
    "## Make sure to check how many people and votes, relative to the total matrix, you are losing given min vote threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CKPYtgNCdXa"
   },
   "source": [
    "### Full participants * comments matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnBllqIVCdXd"
   },
   "source": [
    "Some things to notice about the matrix: comments are submitted over time, so participants who do not return will only have voted on the statements which were avialable when they arrived. \n",
    "\n",
    "Long horizontal lines: participants who do return show up as a horizontal line sticking out into otherwise blank areas\n",
    "\n",
    "Blank vertical lines: most likely statements which were moderated out of the conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 837
    },
    "id": "qxkndwk4CdXe",
    "outputId": "64fd5a77-8229-4093-b1ea-5e5ad9bf8f4f"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,14))\n",
    "sns.heatmap(vals_all_in, center=0, cmap=\"RdBu\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XL6pztUDCdXh"
   },
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def sparsity_aware_dist(a, b):\n",
    "    n_both_seen = len(a) - (np.isnan(a) | np.isnan(b)).sum()\n",
    "    return (n_both_seen - (a == b).sum() + 1) / (n_both_seen + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYf_45cdCdXk"
   },
   "outputs": [],
   "source": [
    "def polis_pca(dataframe, components):\n",
    "    pca_object = PCA(n_components=components) ## pca is apparently different, it wants \n",
    "    pca_object = pca_object.fit(dataframe.T) ## .T transposes the matrix (flips it)\n",
    "    coords = pca_object.components_.T ## isolate the coordinates and flip \n",
    "    explained_variance = pca_object.explained_variance_ratio_\n",
    "\n",
    "    return coords, explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRXtaihTCdXm"
   },
   "outputs": [],
   "source": [
    "def polis_umap(dataframe, neighbors):\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=neighbors,\n",
    "        metric=sparsity_aware_dist,\n",
    "        init='random',\n",
    "        min_dist=0.1,\n",
    "        spread=1.0,\n",
    "        local_connectivity=3.0,\n",
    "    )\n",
    "    embedding = reducer.fit_transform(dataframe.values)\n",
    "    # embedding.shape\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPi4Mx5uCdXo"
   },
   "outputs": [],
   "source": [
    "def c(comment, coords):\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    plt.sca(ax)\n",
    "    colorMap = {-1:'#A50026', 1:'#313695', 0:'#FEFEC050'}\n",
    "    ax.scatter(\n",
    "        x=coords[:,0],\n",
    "        y=coords[:,1],\n",
    "        c=vals[str(comment)].apply(lambda x: colorMap[x]),\n",
    "        s=10\n",
    "    )\n",
    "    ax.set_title(\"\\n\".join(wrap(str(comment) + \"  \" + str(df_comments['comment-body'][comment]))), fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_d_y7FtCdXv"
   },
   "outputs": [],
   "source": [
    "## Thanks to https://github.com/ciortanmadalina/high_noise_clustering/blob/master/graph-partitioning-louvain.ipynb\n",
    "\n",
    "def polis_leiden(dataframe, neighbors):\n",
    "    A = kneighbors_graph(\n",
    "        dataframe.values, \n",
    "        neighbors, \n",
    "        mode=\"connectivity\", \n",
    "        metric=sparsity_aware_dist, \n",
    "        p=3, \n",
    "        metric_params=None, \n",
    "        include_self=True, \n",
    "        n_jobs=None\n",
    "    )\n",
    "    print(dataframe.values.shape)\n",
    "    print(A.shape)\n",
    "\n",
    "    sources, targets = A.nonzero()\n",
    "    weights = A[sources, targets]\n",
    "    if isinstance(weights, np.matrix): # ravel data\n",
    "            weights = weights.A1\n",
    "\n",
    "    g = ig.Graph(directed=False)\n",
    "    g.add_vertices(A.shape[0])  # each observation is a node\n",
    "    edges = list(zip(sources, targets))\n",
    "    g.add_edges(edges)\n",
    "    g.es['weight'] = weights\n",
    "    weights = np.array(g.es[\"weight\"]).astype(np.float64)\n",
    "\n",
    "    part = leidenalg.find_partition(\n",
    "        g, \n",
    "        leidenalg.ModularityVertexPartition\n",
    "    );\n",
    "\n",
    "    leidenClusters = np.array(part.membership)\n",
    "    leidenClustersStr = [str(i) for i in leidenClusters] \n",
    "\n",
    "    from collections import Counter\n",
    "    print(len(Counter(part.membership).keys()))\n",
    "\n",
    "    #df[\"leiden\"] = leidenClustersStr\n",
    "    \n",
    "    return leidenClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQHZ0CYM5yLC"
   },
   "outputs": [],
   "source": [
    "def polis_subconversation(dataframe, comments):\n",
    "    coords, explained_variance = polis_pca(dataframe, 2)\n",
    "    print(\"Explained variance:\", explained_variance)\n",
    "\n",
    "    embedding = polis_umap(dataframe, 10)\n",
    "\n",
    "    leidenClusters = polis_leiden(dataframe, 8)\n",
    "\n",
    "\n",
    "    # Show clusters given umap embedding \n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    plt.sca(ax)\n",
    "    ax.scatter(\n",
    "        x=embedding[:,0],\n",
    "        y=embedding[:,1],\n",
    "        c=leidenClusters,\n",
    "        cmap=\"tab20\",\n",
    "        s=5\n",
    "    )\n",
    "    ax.set_title(\"Leiden detected communities in UMAP space\", fontsize=14)\n",
    "\n",
    "\n",
    "    # Show clusters given pca embedding \n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    plt.sca(ax)\n",
    "    ax.scatter(\n",
    "        x=coords[:,0],\n",
    "        y=coords[:,1],\n",
    "        c=leidenClusters,\n",
    "        cmap=\"tab20\",\n",
    "        s=5\n",
    "    )\n",
    "\n",
    "    ax.set_title(\"Leiden detected communities in PCA space\", fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "    # number of votes in pca space\n",
    "    plt.figure(figsize=(7, 5), dpi=80)\n",
    "    plt.scatter(\n",
    "        x=coords[:,0], \n",
    "        y=coords[:,1], \n",
    "        c=metadata['n-votes'],\n",
    "        cmap=\"magma_r\",\n",
    "        s=5\n",
    "    )\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # when did the participant show up? index\n",
    "    plt.figure(figsize=(7, 5), dpi=80)\n",
    "    plt.scatter(\n",
    "        x=coords[:,0], \n",
    "        y=coords[:,1], \n",
    "        c=metadata.index,\n",
    "        cmap=\"magma_r\",\n",
    "        s=5\n",
    "    )\n",
    "    plt.colorbar()\n",
    "    \n",
    "    \n",
    "    # PCA for COMMENTS\n",
    "    \n",
    "    coords, explained_variance = polis_pca(dataframe.T, 2)\n",
    "    \n",
    "    plt.figure(figsize=(7, 5), dpi=80)\n",
    "    plt.scatter(\n",
    "        x=coords[:,0], \n",
    "        y=coords[:,1], \n",
    "#         c=,\n",
    "        cmap=\"magma_r\",\n",
    "        s=5\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "#     plt.colorbar()\n",
    "    \n",
    "#     # Show clustermap\n",
    "#     dataframe['leiden_cluster_assignments'] = leidenClusters\n",
    "#     clusters_by_comments_means = dataframe.groupby('leiden_cluster_assignments').agg('mean')\n",
    "\n",
    "#     #sns.heatmap(clusters_by_comments_means, cmap=\"RdYlBu\")\n",
    "#     sns.clustermap(clusters_by_comments_means, cmap=\"RdYlBu\", figsize=(15,15))\n",
    "\n",
    "    for x in comments:\n",
    "        c(x, coords)\n",
    "        c(x, embedding)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_UxcALNCdX2"
   },
   "outputs": [],
   "source": [
    "def polis_heatmap(__dataframe):\n",
    "    leidenClusters = polis_leiden(__dataframe, 8)\n",
    "\n",
    "    # Show clustermap\n",
    "    __dataframe['leiden_cluster_assignments'] = leidenClusters\n",
    "    clusters_by_comments_means = __dataframe.groupby('leiden_cluster_assignments').agg('mean').T\n",
    "\n",
    "    index_to_label = df_comments['comment-body'].to_dict() # {index: label}\n",
    "\n",
    "    clustergrid = sns.clustermap(clusters_by_comments_means, cmap=\"RdBu\", figsize=(10,10), )\n",
    "\n",
    "    ax = clustergrid.ax_heatmap\n",
    "    new_labels = [index_to_label[str(idx._text)] for idx in ax.get_yticklabels()] # [ label0, label1, label2, ...]\n",
    "    ax.set_yticklabels(new_labels, rotation=0, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMT37UBWDtcy"
   },
   "source": [
    "### Comparison of k-neighbor and two-layer graph approaches\n",
    "\n",
    "* Currently a k-neighbor graph of the participant-votes matrix is given as input to the Leiden algorithm for community detection.\n",
    "* However, the participant-votes matrix can be given directly.\n",
    "* Quality of results is low, if the adjacency matrix is given as-is, in one-layer format.\n",
    "    * That is, as a weighted adjacency matrix with 1.0 for positive links, and -1.0 for negative ones.\n",
    "* Quality of results is comparable to k-neighbors graph, if the two-layer structure is explicitly given as input to the Leiden algorithm.\n",
    "    * That is, the weighted adjacency matrix is split in two, G_pos and G_neg.\n",
    "    * G_pos is an unweighted version of the same matrix, with 1.0 where positive links exists and 0.0 otherwise.\n",
    "    * G_neg is defined similarly for the negative links.\n",
    "    * Both layers are given as input to Leiden, with layer_weights=[1 -1] \n",
    "    * (i.e. preferring positive connections and penalizing negative ones.)\n",
    "\n",
    "* Future work: utilization of bipartite form\n",
    "    * A bipartite graph can also be expressed in three layers and inputted to the Leiden algo.\n",
    "    * Together with pos/neg links, makes for a total six layers.\n",
    "    * A sample implementation can be found below, yet quality is low.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCzhQopug8Y8"
   },
   "outputs": [],
   "source": [
    "def generate_pos_neg_graphs(dataframe):\n",
    "    # Generate two-layer multiplex bipartite graphs using dataframe data\n",
    "    #\n",
    "    # Parameters:\n",
    "    #   dataframe: a participant-votes dataframe\n",
    "    #\n",
    "    # Returns:\n",
    "    #   G_pos: participant-votes bipartite graph of positive votes (i.e. positive layer)\n",
    "    #   G_neg: participant-votes bipartite graph of negative votes (i.e. negative layer)\n",
    "    A = dataframe.values\n",
    "    \n",
    "    # Convert 'bipartite' matrix A (rows=participants, columns=votes) \n",
    "    # into unipartite form, by adding votes rows and participant columns.\n",
    "    #\n",
    "    # The new rectangular matrix A_sym can be written in block form as:\n",
    "    # A_sym = [ 0     A ]\n",
    "    #         [ A.T   0 ]\n",
    "    A_sym = np.block([[np.zeros((A.shape[0], A.shape[0])), A], [np.transpose(A), np.zeros((A.shape[1], A.shape[1]))] ])\n",
    "\n",
    "    # G_pos includes only positive edges (positive weights in A_sym)\n",
    "    G_pos = ig.Graph.Adjacency((A_sym > 0).tolist(), mode=\"undirected\")\n",
    "    # G_neg includes only negative edges (negative weights in A_sym)\n",
    "    G_neg = ig.Graph.Adjacency((A_sym < 0).tolist(), mode=\"undirected\")\n",
    "\n",
    "    # Partition of nodes in two classes (bipartite graph) - optional info\n",
    "    # Class 0: participants - first A.shape[0] nodes\n",
    "    # Class 1: votes - subsequent A.shape[1] nodes\n",
    "    G_pos.vs['type'] = np.block([np.ones(A.shape[0]), np.zeros(A.shape[1])])\n",
    "    G_neg.vs['type'] = np.block([np.ones(A.shape[0]), np.zeros(A.shape[1])])\n",
    "\n",
    "    return G_pos, G_neg\n",
    "\n",
    "def generate_dual_graph(dataframe):\n",
    "    # Generate one-layer bipartite graph using dataframe data\n",
    "    #\n",
    "    # Parameters:\n",
    "    #   dataframe: a participant-votes dataframe\n",
    "    #\n",
    "    # Returns:\n",
    "    #   G: participant-votes bipartite graph of pos/neg votes (i.e. one layer)\n",
    "\n",
    "    A = dataframe.values\n",
    "    \n",
    "    A_sym = np.block([[np.zeros((A.shape[0], A.shape[0])), A], [np.transpose(A), np.zeros((A.shape[1], A.shape[1]))] ])\n",
    "    g = ig.Graph.Adjacency((A_sym != 0).tolist(), mode=\"undirected\")\n",
    "    g.es['weight'] = A_sym[A_sym.nonzero()] # Possibly not needed\n",
    "\n",
    "    # Partition of nodes in two classes (bipartite graph) - optional info\n",
    "    # Class 0: participants - first A.shape[0] nodes\n",
    "    # Class 1: votes - subsequent A.shape[1] nodes\n",
    "    g.vs['type'] = np.block([np.ones(A.shape[0]), np.zeros(A.shape[1])])\n",
    "\n",
    "    # Naming nodes for easy retrieval\n",
    "    for idx, v in enumerate(g.vs):\n",
    "      v['name'] = idx\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTzNFr6MaR1w"
   },
   "outputs": [],
   "source": [
    "def generate_kneighbors_graph(dataframe, neighbors):\n",
    "    # Generate k-neighbors unipartite graphs using dataframe data\n",
    "    #\n",
    "    # Parameters:\n",
    "    #   dataframe: a participant-votes dataframe\n",
    "    #   neighbors (int): number of neighbors (k)\n",
    "    #\n",
    "    # Returns:\n",
    "    #   G: participant-participant unipartite graph of k-neighbors connectivity\n",
    "\n",
    "    A = kneighbors_graph(\n",
    "        dataframe.values, \n",
    "        neighbors, \n",
    "        mode=\"connectivity\", \n",
    "        metric=sparsity_aware_dist, \n",
    "        p=3, \n",
    "        metric_params=None, \n",
    "        include_self=True, \n",
    "        n_jobs=None\n",
    "    )\n",
    "    print(\"Dataframe shape: {}\".format(dataframe.values.shape))\n",
    "    print(\"Kneighbor graph shape: {}\".format(A.shape))\n",
    "\n",
    "    sources, targets = A.nonzero()\n",
    "    weights = A[sources, targets]\n",
    "    if isinstance(weights, np.matrix): # ravel data\n",
    "            weights = weights.A1\n",
    "\n",
    "    g = ig.Graph(directed=False)\n",
    "    g.add_vertices(A.shape[0])  # each observation is a node\n",
    "    edges = list(zip(sources, targets))\n",
    "    g.add_edges(edges)\n",
    "    g.es['weight'] = weights\n",
    "    weights = np.array(g.es[\"weight\"]).astype(np.float64)\n",
    "\n",
    "    return g\n",
    "\n",
    "#generate_kneighbors_graph(vals_all_in, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbk8JEVnWHz-"
   },
   "outputs": [],
   "source": [
    "def polis_leiden_onelayer(graph, optimizationfunction):\n",
    "    # Calculate clusters using Leiden algorithm for one-layer graph \n",
    "    #\n",
    "    # Parameters:\n",
    "    #   graph: the graph to detect communities on\n",
    "    #   optimizationfunction: the optimization function for Leiden alg\n",
    "    #\n",
    "    # Returns:\n",
    "    #   leidenClusters: the partition sequence detected by Leiden alg\n",
    "\n",
    "    part = leidenalg.find_partition(\n",
    "        graph, \n",
    "        optimizationfunction\n",
    "    );\n",
    "\n",
    "    leidenClusters = np.array(part.membership)\n",
    "\n",
    "    # Print total nr of communities detected\n",
    "    from collections import Counter\n",
    "    print(\"Nr of communities detected: {}\".format(len(Counter(part.membership).keys())))\n",
    "    \n",
    "    return leidenClusters\n",
    "\n",
    "def polis_leiden_onelayer_iterative(graph, optimizationfunction, n_iterations):\n",
    "    # Calculate clusters using Leiden algorithm for one-layer graph, iteratively\n",
    "    #\n",
    "    # Parameters:\n",
    "    #   graph: the graph to detect communities on\n",
    "    #   optimizationfunction: the optimization function for Leiden alg\n",
    "    #   n_iterations (int): number of iterations\n",
    "    #\n",
    "    # Returns:\n",
    "    #   leidenClusters: the partition sequence detected by Leiden alg\n",
    "\n",
    "    part = optimizationfunction(graph)\n",
    "    diff = leidenalg.Optimiser().optimise_partition(part, n_iterations)\n",
    "\n",
    "    leidenClusters = np.array(part.membership)\n",
    "\n",
    "    # Print total nr of communities detected\n",
    "    from collections import Counter\n",
    "    print(\"Nr of communities detected after {} iterations: {}\".format(\n",
    "        n_iterations, \n",
    "        len(Counter(part.membership).keys())))\n",
    "    \n",
    "    return leidenClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNEzBtu0j88N"
   },
   "outputs": [],
   "source": [
    "def polis_leiden_twolayers(G_pos, G_neg, optimizationfunction):\n",
    "    # Calculate clusters using Leiden algorithm for two-layer graph \n",
    "    #\n",
    "    # Parameters:\n",
    "    #   G_pos: the positive (layer of the) graph\n",
    "    #   G_neg: the negative (layer of the) graph\n",
    "    #   optimizationfunction: the optimization function for Leiden alg\n",
    "    #\n",
    "    # Returns:\n",
    "    #   leidenClusters: the partition sequence detected by Leiden alg\n",
    "\n",
    "    optimiser = leidenalg.Optimiser()\n",
    "    partition_pos = optimizationfunction(G_pos)\n",
    "    partition_neg = optimizationfunction(G_neg)\n",
    "    # Both layers are fed into Leiden. layer_weights=[1,-1] penalize negative layer\n",
    "    # See also 4.1.1 in https://readthedocs.org/projects/leidenalg/downloads/pdf/latest/ \n",
    "    diff = optimiser.optimise_partition_multiplex(\n",
    "                   partitions=[partition_pos, partition_neg],\n",
    "                   layer_weights=[1,-1])\n",
    "    \n",
    "    leidenClusters = np.array(partition_pos.membership)\n",
    "\n",
    "    # Print the sequence and nr of communities detected\n",
    "    from collections import Counter\n",
    "    print(\"Nr of communities detected: {}\".format(len(Counter(partition_pos.membership).keys())))\n",
    "    print(\"Seq of communities detected: {}\".format(Counter(np.array(partition_pos.membership))))\n",
    "\n",
    "    return leidenClusters\n",
    "'''\n",
    "from collections import Counter\n",
    "G_pos, G_neg = generate_pos_neg_graphs(vals_all_in)\n",
    "lc = polis_leiden_twolayers(G_pos, G_neg, leidenalg.ModularityVertexPartition)\n",
    "lc_participants = lc[:vals_all_in.shape[0]]\n",
    "'''\n",
    "def polis_leiden_twolayers_iterative(G_pos, G_neg, optimizationfunction, n_iterations):\n",
    "    # Calculate clusters using Leiden algorithm for two-layer graph, iterative\n",
    "    #\n",
    "    # Parameters:\n",
    "    #   G_pos: the positive (layer of the) graph\n",
    "    #   G_neg: the negative (layer of the) graph\n",
    "    #   optimizationfunction: the optimization function for Leiden alg\n",
    "    #   n_iterations (int): number of iterations\n",
    "    #\n",
    "    # Returns:\n",
    "    #   leidenClusters: the partition sequence detected by Leiden alg\n",
    "\n",
    "    optimiser = leidenalg.Optimiser()\n",
    "    partition_pos = optimizationfunction(G_pos)\n",
    "    partition_neg = optimizationfunction(G_neg)\n",
    "    diff = optimiser.optimise_partition_multiplex(\n",
    "                   partitions=[partition_pos, partition_neg],\n",
    "                   layer_weights=[1,-1],\n",
    "                   n_iterations=n_iterations)\n",
    "    \n",
    "    leidenClusters = np.array(partition_pos.membership)\n",
    "\n",
    "    # Print the sequence and nr of communities detected\n",
    "    from collections import Counter\n",
    "    print(\"Nr of communities detected after {} iterations: {}\".format(\n",
    "        n_iterations,\n",
    "        len(Counter(partition_pos.membership).keys())))\n",
    "    print(\"Seq of communities detectedafter {} iterations: {}\".format(\n",
    "        n_iterations, \n",
    "        Counter(np.array(partition_pos.membership))))\n",
    "\n",
    "    return leidenClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IL-_M9GDn6EA"
   },
   "outputs": [],
   "source": [
    "def polis_leiden_twolayers_bipartite(G_pos, G_neg):\n",
    "    # DO NOT USE! Currently not working as intended!\n",
    "    #\n",
    "    # Calculate clusters using Leiden algorithm for two-layer bipartite graph\n",
    "    #\n",
    "    # NB: Unlike the other polis_leiden_* functions, this always optimizes using modularity\n",
    "    #\n",
    "    # Parameters:\n",
    "    #   G_pos: the positive (layer of the) graph\n",
    "    #   G_neg: the negative (layer of the) graph\n",
    "    #\n",
    "    # Returns:\n",
    "    #   leidenClusters: the partition sequence detected by Leiden alg\n",
    "\n",
    "    optimiser = leidenalg.Optimiser()\n",
    "    part_pos_0, part_pos_1, part_pos_2 = leidenalg.CPMVertexPartition.Bipartite(G_pos, 0.01, 0, 0, True);\n",
    "    part_neg_0, part_neg_1, part_neg_2 = leidenalg.CPMVertexPartition.Bipartite(G_neg, 0.01, 0, 0, True);\n",
    "    # See also https://leidenalg.readthedocs.io/en/stable/reference.html#leidenalg.CPMVertexPartition.Bipartite\n",
    "    diff = optimiser.optimise_partition_multiplex(\n",
    "        partitions=[part_pos_0, part_pos_1, part_pos_2, part_neg_0, part_neg_1, part_neg_2], \n",
    "        layer_weights=[1,-1,-1,-1,-1,-1]\n",
    "        )\n",
    "    \n",
    "    leidenClusters = np.array(part_pos_0.membership)\n",
    "\n",
    "    # Print the sequence and nr of communities detected\n",
    "    from collections import Counter\n",
    "    print(\"Nr of communities detected: {}\".format(len(Counter(part_pos_0.membership).keys())))\n",
    "    print(\"Seq of communities detected: {}\".format(Counter(np.array(part_pos_0.membership))))\n",
    "\n",
    "    return leidenClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vhkKwPNGCdX0",
    "outputId": "cfdca864-3ae8-47e2-9d16-ed3b35c48fa9"
   },
   "outputs": [],
   "source": [
    "def show_embedding(space, color, title):\n",
    "    # Helper function to plot community data as color on an embedding space\n",
    "    #\n",
    "    # Parameters:\n",
    "    #   space: the embedding space to plot data on (e.g. UMAP, PCA)\n",
    "    #   color: the community data to plot as color in the embedding\n",
    "    #   title: the plot's title\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    plt.sca(ax)\n",
    "    # NB: palette contains only 20 colors ('tab20')\n",
    "    # (User needs to artificially limit communities to <20)\n",
    "    ax.scatter(\n",
    "        x=space[:,0],\n",
    "        y=space[:,1],\n",
    "        c=color,\n",
    "        cmap=\"tab20\",\n",
    "        s=5\n",
    "    )\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "def polis_subconversation_alt(dataframe, comments):\n",
    "    # Experimentation with and plotting of different community detection \n",
    "    # scenarios, on variations of the same dataset. \n",
    "    #\n",
    "    # (Approximate stand-in for original polis_subconversation function)\n",
    "    #\n",
    "    # Parameters:\n",
    "    #   dataframe: a participant-votes dataframe\n",
    "    #   comments: not used here\n",
    "\n",
    "    embedding = polis_umap(dataframe, 10) \n",
    "    coords, explained_variance = polis_pca(dataframe, 2)\n",
    "    print(\"Explained variance:\", explained_variance)\n",
    "\n",
    "    #leidenClusters = polis_leiden(dataframe, 8)\n",
    "    #show_embedding(embedding, leidenClusters, \"Leiden detected communities in UMAP space\")\n",
    "    #show_embedding(coords, leidenClusters, \"Leiden detected communities in PCA space\")\n",
    "\n",
    "    ########################\n",
    "    # K-neighbors, one layer\n",
    "    ########################\n",
    "    G_kneighbors = generate_kneighbors_graph(dataframe, 8)\n",
    "    lc = polis_leiden_onelayer(G_kneighbors, leidenalg.ModularityVertexPartition)\n",
    "    show_embedding(embedding, lc, \"Leiden, UMAP, kneighbor graph, one layer, modularity\")\n",
    "    show_embedding(coords, lc, \"Leiden, PCA, kneighbor graph, , one layer, modularity\")\n",
    "\n",
    "    '''\n",
    "    lc = polis_leiden_onelayer_iterative(G_kneighbors, leidenalg.ModularityVertexPartition, 10)\n",
    "    show_embedding(embedding, lc, \"Leiden, UMAP, kneighbor graph, one layer, modularity, iterative\")\n",
    "    show_embedding(coords, lc, \"Leiden, PCA, kneighbor graph, , one layer, modularity, iterative\")\n",
    "\n",
    "    lc = polis_leiden_onelayer(G_kneighbors, leidenalg.CPMVertexPartition)\n",
    "    show_embedding(embedding, lc, \"Leiden, UMAP, kneighbor graph, one layer, CPM\")\n",
    "    show_embedding(coords, lc, \"Leiden, PCA, kneighbor graph, one layer, CPM\")\n",
    "\n",
    "    lc = polis_leiden_onelayer_iterative(G_kneighbors, leidenalg.CPMVertexPartition, 10)\n",
    "    show_embedding(embedding, lc, \"Leiden, UMAP, kneighbor graph, one layer, CPM, iterative\")\n",
    "    show_embedding(coords, lc, \"Leiden, PCA, kneighbor graph, one layer, CPM, iterative\")\n",
    "    '''\n",
    "\n",
    "    #############################\n",
    "    # Adjacency matrix, one layer\n",
    "    #############################\n",
    "    G = generate_dual_graph(dataframe)\n",
    "    lc = polis_leiden_onelayer(G, leidenalg.ModularityVertexPartition)\n",
    "    # Adjustment in case of > 20 communities (i.e. limited color palette)\n",
    "    lc_show = []\n",
    "    for x in lc[:embedding.shape[0]]:\n",
    "      lc_show.append(x) if x < 20 else lc_show.append(20)\n",
    "    show_embedding(embedding, lc_show, \"Leiden, UMAP, adjacency matrix , one layer, modularity\")\n",
    "    show_embedding(coords, lc_show, \"Leiden, PCA, adjacency matrix, one layer, modularity\")\n",
    "\n",
    "    '''\n",
    "    lc = polis_leiden_onelayer_iterative(G, leidenalg.ModularityVertexPartition, 10)\n",
    "    show_embedding(embedding, lc[:embedding.shape[0]], \"Leiden, UMAP, adjacency matrix, one layer, modularity, iterative\")\n",
    "    show_embedding(coords, lc[:coords.shape[0]], \"Leiden, PCA, adjacency matrix, one layer, modularity, iterative\")\n",
    "\n",
    "    lc = polis_leiden_onelayer(G, leidenalg.CPMVertexPartition)\n",
    "    show_embedding(embedding, lc[:embedding.shape[0]], \"Leiden, UMAP, adjacency matrix, one layer, CPM\")\n",
    "    show_embedding(coords, lc[:coords.shape[0]], \"Leiden, PCA, adjacency matrix, one layer, CPM\")\n",
    "\n",
    "    lc = polis_leiden_onelayer_iterative(G, leidenalg.CPMVertexPartition, 10)\n",
    "    show_embedding(embedding, lc[:embedding.shape[0]], \"Leiden, UMAP, adjacency matrix, one layer, CPM, iterative\")\n",
    "    show_embedding(coords, lc[:coords.shape[0]], \"Leiden, PCA, adjacency matrix, one layer, CPM, iterative\")\n",
    "    '''\n",
    "\n",
    "    ##############################\n",
    "    # Adjacency matrix, two layers\n",
    "    ##############################\n",
    "    G_pos, G_neg = generate_pos_neg_graphs(dataframe)\n",
    "    lc = polis_leiden_twolayers(G_pos, G_neg, leidenalg.ModularityVertexPartition)\n",
    "    # Adjustment in case of > 20 communities (i.e. limited color pallete)\n",
    "    lc_show = []\n",
    "    for x in lc[:embedding.shape[0]]:\n",
    "      lc_show.append(x) if x < 20 else lc_show.append(20)\n",
    "    show_embedding(embedding, lc_show, \"Leiden, UMAP, adjacency matrix , two layers, modularity\")\n",
    "    show_embedding(coords, lc_show, \"Leiden, PCA, adjacency matrix, two layers, modularity\")\n",
    "    '''\n",
    "    lc = polis_leiden_twolayers_iterative(G_pos, G_neg, leidenalg.ModularityVertexPartition, 10)\n",
    "    show_embedding(embedding, lc[:embedding.shape[0]], \"Leiden, UMAP, adjacency matrix, two layers, modularity, iterative\")\n",
    "    show_embedding(coords, lc[:coords.shape[0]], \"Leiden, PCA, adjacency matrix, two layers, modularity, iterative\")\n",
    "    \n",
    "    lc = polis_leiden_twolayers(G_pos, G_neg, leidenalg.CPMVertexPartition)\n",
    "    show_embedding(embedding, lc[:embedding.shape[0]], \"Leiden, UMAP, adjacency matrix, two layers, CPM\")\n",
    "    show_embedding(coords, lc[:coords.shape[0]], \"Leiden, PCA, adjacency matrix, two layers, CPM\")\n",
    "    \n",
    "    lc = polis_leiden_twolayers_iterative(G_pos, G_neg, leidenalg.CPMVertexPartition, 10)\n",
    "    show_embedding(embedding, lc[:embedding.shape[0]], \"Leiden, UMAP, adjacency matrix, two layers, CPM, iterative\")\n",
    "    show_embedding(coords, lc[:coords.shape[0]], \"Leiden, PCA, adjacency matrix, two layers, CPM, iterative\")\n",
    "    '''\n",
    "\n",
    "    #########################################\n",
    "    # Adjacency matrix, two layers, bipartite\n",
    "    #########################################\n",
    "    '''\n",
    "    G_pos, G_neg = generate_pos_neg_graphs(dataframe)\n",
    "    lc = polis_leiden_twolayers_bipartite(G_pos, G_neg)\n",
    "    # Adjustment in case of > 20 communities (i.e. limited color palette)\n",
    "    lc_show = []\n",
    "    for x in lc[:embedding.shape[0]]:\n",
    "      lc_show.append(x) if x < 20 else lc_show.append(20)\n",
    "    show_embedding(embedding, lc_show, \"Leiden, UMAP, adjacency matrix , two layers, modularity, bipartite\")\n",
    "    show_embedding(coords, lc_show, \"Leiden, PCA, adjacency matrix, two layers, modularity, bipartite\")\n",
    "    '''\n",
    "\n",
    "    return lc, G\n",
    "\n",
    "leidenClust, G = polis_subconversation_alt(vals_all_in, high_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmhJWdiz9OI4"
   },
   "source": [
    "## Looking into comments:\n",
    "\n",
    "* which communities they belong to, according to the last Leiden run above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKDqdSW6Tv4U",
    "outputId": "0c929b25-d132-4203-9ecf-f56b1e7453a7"
   },
   "outputs": [],
   "source": [
    "index_to_label = df_comments['comment-body'].to_dict() # {index: label}\n",
    "print(index_to_label)\n",
    "print(leidenClust)\n",
    "from collections import Counter\n",
    "print(\"Nr of comments/statements belonging to communities: {}\".format(Counter(leidenClust[1006:])))\n",
    "\n",
    "statements_indx = [index_to_label[str(v[\"name\"]-1006)] for v in G.vs if v[\"type\"] == 0.0]\n",
    "statements = [v[\"name\"] for v in G.vs if v[\"type\"] == 0.0]\n",
    "import pprint\n",
    "pprint.pprint(sorted(zip(leidenClust[statements],statements_indx)))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtnxTBdhCdX5"
   },
   "source": [
    "# Is the space explained by how much people vote?\n",
    "\n",
    "In this chart, we take the PCA coordinates and color the participant locations by the number of total votes. Hopefully, it looks random. If it doesn't, we might imagine the following scenario:\n",
    "\n",
    "1. 1000 people vote, and there are very few controversial statements. They do not return.\n",
    "2. 1 person submits a statement which is incredibly controversial. \n",
    "3. 1000 more people vote, the space begins to take on structure, PCA is closely linked to vote count.\n",
    "\n",
    "We know this scenario - that voters don't see controversial comments - happens. Polis mitigates in two ways:\n",
    "* polis eliminates participants who don't vote at least 7 times from the analysis\n",
    "* polis shows several highly controversial comments (large egeinvalue) in the first 10 comments participants see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "id": "X8fSKIM3CdX5",
    "outputId": "bbe9fddb-90e4-47d5-dd5a-3c198a434e09"
   },
   "outputs": [],
   "source": [
    "coords, embedding = polis_pca(vals_all_in, 2)\n",
    "\n",
    "plt.figure(figsize=(7, 5), dpi=80)\n",
    "plt.scatter(\n",
    "    x=coords[:,0], \n",
    "    y=coords[:,1], \n",
    "    c=metadata['n-votes'],\n",
    "    cmap=\"magma_r\",\n",
    "    s=5\n",
    ")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eA6QoEJ_CdX_"
   },
   "source": [
    "# High Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IkJVBygaCdX_",
    "outputId": "ee3f063f-7103-4448-cc1b-73b212d4b50a"
   },
   "outputs": [],
   "source": [
    "polis_subconversation(vals_all_in, high_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-02rAHwCdYC"
   },
   "source": [
    "# Consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xECrTFg0CdYC"
   },
   "outputs": [],
   "source": [
    "polis_subconversation(vals_all_in, statements_consensus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9tNnrgVCdYE"
   },
   "source": [
    "# Opiods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rqkv5Qk6CdYF"
   },
   "outputs": [],
   "source": [
    "polis_heatmap(vals_opiods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2NlcFpICdYJ"
   },
   "outputs": [],
   "source": [
    "polis_subconversation_alt(vals_opiods, statements_opiods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yePYI9CECdYO"
   },
   "source": [
    "# Homelessness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyidTl9FCdYO"
   },
   "outputs": [],
   "source": [
    "def polis_heatmap(__dataframe):\n",
    "  leidenClusters = polis_leiden(__dataframe, 8)\n",
    "\n",
    "  # Show clustermap\n",
    "  __dataframe['leiden_cluster_assignments'] = leidenClusters\n",
    "\n",
    "  index_to_label = df_comments['comment-body'].to_dict() # {index: label}\n",
    "\n",
    "  clustergrid = sns.clustermap(vals_homelessness, cmap=\"RdBu\", figsize=(10,10), )\n",
    "\n",
    "  ax = clustergrid.ax_heatmap\n",
    "  new_labels = [index_to_label[str(idx._text)] for idx in ax.get_xticklabels()] # [ label0, label1, label2, ...]\n",
    "  ax.set_xticklabels(new_labels, rotation=90, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aF2oKjXrCdYR"
   },
   "outputs": [],
   "source": [
    "polis_heatmap(vals_homelessness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CY507aA4CdYX"
   },
   "outputs": [],
   "source": [
    "polis_subconversation(vals_homelessness, statements_homelessness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6whvoknrCdYe"
   },
   "source": [
    "# All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YS0yVClBCdYf"
   },
   "outputs": [],
   "source": [
    "# coords, explained_variance = polis_pca(vals_all_in, 2)\n",
    "# print(\"Explained variance:\", explained_variance)\n",
    "\n",
    "# embedding = polis_umap(vals_all_in, 4)\n",
    "\n",
    "# leidenClusters = polis_leiden(vals_all_in, 8)\n",
    "\n",
    "# # Show clusters given umap embedding \n",
    "# fig, ax = plt.subplots(figsize=(7,5))\n",
    "# plt.sca(ax)\n",
    "# ax.scatter(\n",
    "#     x=embedding[:,0],\n",
    "#     y=embedding[:,1],\n",
    "#     c=leidenClusters,\n",
    "#     cmap=\"tab20\",\n",
    "#     s=5\n",
    "# )\n",
    "\n",
    "# ax.set_title(\"Leiden detected communities in UMAP space\", fontsize=14)\n",
    "# plt.show()\n",
    "\n",
    "# for x in statements_all_in:\n",
    "#     if int(x) < 5:\n",
    "#         c(x, coords)\n",
    "#         c(x, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irENHxxUCdYv"
   },
   "outputs": [],
   "source": [
    "# leidenClusters = polis_leiden(vals_opiods, 8)\n",
    "\n",
    "# # Show clustermap\n",
    "# vals_opiods['leiden_cluster_assignments'] = leidenClusters\n",
    "# clusters_by_comments_means = vals_opiods.groupby('leiden_cluster_assignments').agg('mean').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAxttIUUCdYx"
   },
   "outputs": [],
   "source": [
    "# index_to_label = df_comments['comment-body'].to_dict() # {index: label}\n",
    "\n",
    "# clustergrid = sns.clustermap(clusters_by_comments_means, cmap=\"RdYlBu\", figsize=(15,15), )\n",
    "\n",
    "# ax = clustergrid.ax_heatmap\n",
    "# new_labels = [index_to_label[str(idx._text)] for idx in ax.get_yticklabels()] # [ label0, label1, label2, ...]\n",
    "# ax.set_yticklabels(new_labels, rotation=0, fontsize=16)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of American Assembly BG analysis v0.7 subconversation, every vote.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
